encoder: hubert
encoder_conf:
    output_size: 512
    hubert_url: https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt # hubert base noFT
    # hubert_url: https://dl.fbaipublicfiles.com/hubert/hubert_large_ll60k.pt # hubert large noFT
    # hubert_url: https://dl.fbaipublicfiles.com/hubert/hubert_xtralarge_ll60k.pt # hubert xlarge noFT
    hubert_dir_path:  # hubert base noFT
    # hubert_dir_path: # hubert large noFT
    # hubert_dir_path: # hubert xlarge noFT
    normalize_before: True
    freeze_finetune_updates: 0

# CTC, no attention
model_conf:
    ctc_weight: 1.0
    lsm_weight: 0.1
    length_normalized_loss: false
ctc_conf:
    ignore_nan_grad: true

# plot related
use_matplotlib: true
num_att_plot: 0

# model selection
best_model_criterion:
-  - valid
   - loss
   - min
keep_nbest_models: 5

seed: 2022
input_size: 1
normalize: none
use_amp: true
allow_variable_data_keys: true
init: none

# -------------hyper-paramters below might should be specified depend on specific corpus-----------------#

# minibatch related
batch_type: numel
batch_bins: 
valid_batch_type: numel
valid_batch_bins: # large valid_batch_bins lead to large GPU memory, we can set it the half of batch_bins (training)

# optimization
accum_grad: 1
max_epoch: 100
patience: 10
optim: adam
optim_conf:
    lr: 0.0005
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 100

# LoRA finetune related
use_adapter: true
adapter: lora
save_strategy: required_grad_only
adapter_conf:
    rank: 32
    alpha: 32
    dropout_rate: 0.05
    target_modules: ["k_proj", "v_proj", "q_proj", "out_proj", "fc1", "fc2"]
