
encoder: hubert
encoder_conf:
    output_size: 512
    hubert_url: https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt # hubert base noFT
    # hubert_url: https://dl.fbaipublicfiles.com/hubert/hubert_large_ll60k.pt # hubert large noFT
    # hubert_url: https://dl.fbaipublicfiles.com/hubert/hubert_xtralarge_ll60k.pt # hubert xlarge noFT
    hubert_dir_path: /media/rosie/d921a251-72e5-45d8-9e41-0309cf76c6b4/espnet_pre_trained_models/from_github/hubert-base-95M-960h-noFT # hubert base noFT
    # hubert_dir_path: /media/rosie/d921a251-72e5-45d8-9e41-0309cf76c6b4/espnet_pre_trained_models/from_github/hubert-large-316M-60k-noFT # hubert large noFT
    # hubert_dir_path: /media/rosie/d921a251-72e5-45d8-9e41-0309cf76c6b4/espnet_pre_trained_models/from_github/hubert-xlarge-1B-60k-noFT # hubert xlarge noFT
    normalize_before: True
    freeze_finetune_updates: 0

# CTC, no attention
model_conf:
    ctc_weight: 1.0
    lsm_weight: 0.1
    length_normalized_loss: false
ctc_conf:
    ignore_nan_grad: true

# plot related
use_matplotlib: true
num_att_plot: 0

# model selection
best_model_criterion:
-  - valid
   - loss
   - min
keep_nbest_models: 5

seed: 2022
input_size: 1
normalize: none
use_amp: true
allow_variable_data_keys: true
init: none

# -------------hyper-paramters below might should be specified depend on specific corpus-----------------#

# minibatch related
batch_type: numel
batch_bins: 
valid_batch_type: numel
valid_batch_bins: # large valid_batch_bins lead to large GPU memory, we can set it the half of batch_bins (training)

# optimization
accum_grad: 1
max_epoch: 100
patience: 10
optim: adam
optim_conf:
    lr: 0.00008 # for CDSD, 8e-5 is good; for Librilight, 1e-4
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 100
